{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "mnist_data = fetch_openml(\"mnist_784\")\n",
    "x = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_labels = 10\n",
    "examples = y.shape[0]\n",
    "y_new = one_hot(y.astype('int32'), num_labels)\n",
    "\n",
    "# Split, reshape, shuffle\n",
    "train_size = 60000\n",
    "test_size = x.shape[0] - train_size\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y_new[:train_size], y_new[train_size:]\n",
    "shuffle_index = np.random.permutation(train_size)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 784) (60000, 10)\n",
      "Test data: (10000, 784) (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, sizes, activation='sigmoid'):\n",
    "    self.sizes = sizes\n",
    "\n",
    "    # Choose activation function\n",
    "    if activation == 'relu':\n",
    "        self.activation = self.relu\n",
    "    elif activation == 'sigmoid':\n",
    "        self.activation = self.sigmoid\n",
    "\n",
    "    # Save all weights\n",
    "    self.params = self.initialize()\n",
    "    # Save all intermediate values, i.e. activations\n",
    "    self.cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(self):\n",
    "    # Number of nodes in each layer\n",
    "    input_layer=self.sizes[0]\n",
    "    hidden_1=self.sizes[1]\n",
    "    hidden_2=self.sizes[2]\n",
    "    output_layer=self.sizes[3]\n",
    "\n",
    "    params = {\n",
    "        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1./hidden_1),\n",
    "        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1./hidden_2),\n",
    "        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1./output_layer)\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(self, x):\n",
    "    self.cache[\"X\"] = x\n",
    "    self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) +\\\n",
    "                                                        self.params[\"b1\"]\n",
    "    self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "    self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) +\\\n",
    "                                                        self.params[\"b2\"]\n",
    "    self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "    return self.cache[\"A2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def softmax(self, x):\n",
    "    # Numerically stable with large exponentials\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(self, y, output):\n",
    "    current_batch_size = y.shape[0]\n",
    "\n",
    "    dZ2 = output - y.T\n",
    "    dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "    db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "    dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "    db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "    return self.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x_train, y_train, x_test, y_test):\n",
    "    for i in range(self.epochs):\n",
    "        # Shuffle\n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            # Batch\n",
    "            begin = j * self.batch_size\n",
    "            end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "            x = x_train_shuffled[begin:end]\n",
    "            y = y_train_shuffled[begin:end]\n",
    "\n",
    "            # Forward\n",
    "            output = self.feed_forward(x)\n",
    "            # Backprop\n",
    "            grad = self.back_propagate(y, output)\n",
    "            # Optimize\n",
    "            self.optimize(l_rate=l_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(self, l_rate=0.1, beta=.9):\n",
    "    '''\n",
    "        Stochatic Gradient Descent (SGD):\n",
    "        θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "        Momentum:\n",
    "        v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "        θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "    '''\n",
    "    if self.optimizer == \"sgd\":\n",
    "        for key in self.params:\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate*self.grads[key]\n",
    "    elif self.optimizer == \"momentum\":\n",
    "        for key in self.params:\n",
    "            self.momemtum_opt[key] = (beta*self.momemtum_opt[key] +\\\n",
    "                                      (1.-beta)*self.grads[key])\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate * self.momemtum_opt[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(self, y, output):\n",
    "    return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        \n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "            \n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialize(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        \n",
    "        params = {\n",
    "            \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "            \"b1\": np.zeros((hidden_layer, 1)) * np.sqrt(1./input_layer),\n",
    "            \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "            \"b2\": np.zeros((output_layer, 1)) * np.sqrt(1./hidden_layer)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momemtum_opt = {\n",
    "            \"W1\": np.zeros(self.params[\"W1\"].shape),\n",
    "            \"b1\": np.zeros(self.params[\"b1\"].shape),\n",
    "            \"W2\": np.zeros(self.params[\"W2\"].shape),\n",
    "            \"b2\": np.zeros(self.params[\"b2\"].shape),\n",
    "        }\n",
    "        return momemtum_opt\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "            y = σ(wX + b)\n",
    "        '''\n",
    "        self.cache[\"X\"] = x\n",
    "        self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) + self.params[\"b1\"]\n",
    "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "        self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) + self.params[\"b2\"]\n",
    "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "        return self.cache[\"A2\"]\n",
    "    \n",
    "    def back_propagate(self, y, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        current_batch_size = y.shape[0]\n",
    "        \n",
    "        dZ2 = output - y.T\n",
    "        dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "        dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m = y.shape[0]\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "                \n",
    "    def optimize(self, l_rate=0.1, beta=.9):\n",
    "        '''\n",
    "            Stochatic Gradient Descent (SGD):\n",
    "            θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "            \n",
    "            Momentum:\n",
    "            v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "            θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "        '''\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                self.params[key] = self.params[key] - l_rate * self.grads[key]\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for key in self.params:\n",
    "                self.momemtum_opt[key] = (beta * self.momemtum_opt[key] + (1. - beta) * self.grads[key])\n",
    "                self.params[key] = self.params[key] - l_rate * self.momemtum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer is currently not support, please use 'sgd' or 'momentum' instead.\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=30, \n",
    "              batch_size=64, optimizer='momentum', l_rate=0.1, beta=.9):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
    "        \n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_test)\n",
    "            test_acc = self.accuracy(y_test, output)\n",
    "            test_loss = self.cross_entropy_loss(y_test, output)\n",
    "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.72s, train acc=0.92, train loss=0.26, test acc=0.93, test loss=0.25\n",
      "Epoch 2: 1.48s, train acc=0.95, train loss=0.19, test acc=0.94, test loss=0.19\n",
      "Epoch 3: 2.25s, train acc=0.95, train loss=0.16, test acc=0.95, test loss=0.17\n",
      "Epoch 4: 3.07s, train acc=0.96, train loss=0.14, test acc=0.96, test loss=0.15\n",
      "Epoch 5: 4.30s, train acc=0.97, train loss=0.12, test acc=0.96, test loss=0.13\n",
      "Epoch 6: 5.53s, train acc=0.97, train loss=0.10, test acc=0.97, test loss=0.12\n",
      "Epoch 7: 6.79s, train acc=0.97, train loss=0.09, test acc=0.97, test loss=0.11\n",
      "Epoch 8: 8.14s, train acc=0.98, train loss=0.08, test acc=0.97, test loss=0.10\n",
      "Epoch 9: 9.43s, train acc=0.98, train loss=0.07, test acc=0.97, test loss=0.10\n",
      "Epoch 10: 10.68s, train acc=0.98, train loss=0.07, test acc=0.97, test loss=0.10\n",
      "Epoch 11: 11.93s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.09\n",
      "Epoch 12: 13.05s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.09\n",
      "Epoch 13: 14.31s, train acc=0.99, train loss=0.05, test acc=0.97, test loss=0.08\n",
      "Epoch 14: 15.49s, train acc=0.99, train loss=0.05, test acc=0.97, test loss=0.09\n",
      "Epoch 15: 17.00s, train acc=0.99, train loss=0.05, test acc=0.98, test loss=0.08\n",
      "Epoch 16: 18.28s, train acc=0.99, train loss=0.05, test acc=0.97, test loss=0.08\n",
      "Epoch 17: 19.44s, train acc=0.99, train loss=0.04, test acc=0.98, test loss=0.08\n",
      "Epoch 18: 20.78s, train acc=0.99, train loss=0.04, test acc=0.98, test loss=0.08\n",
      "Epoch 19: 22.02s, train acc=0.99, train loss=0.04, test acc=0.98, test loss=0.08\n",
      "Epoch 20: 23.16s, train acc=0.99, train loss=0.04, test acc=0.98, test loss=0.08\n",
      "Epoch 21: 24.29s, train acc=0.99, train loss=0.04, test acc=0.98, test loss=0.08\n",
      "Epoch 22: 25.41s, train acc=0.99, train loss=0.03, test acc=0.98, test loss=0.08\n",
      "Epoch 23: 26.49s, train acc=0.99, train loss=0.03, test acc=0.98, test loss=0.08\n",
      "Epoch 24: 27.55s, train acc=0.99, train loss=0.03, test acc=0.98, test loss=0.08\n",
      "Epoch 25: 28.65s, train acc=0.99, train loss=0.03, test acc=0.98, test loss=0.07\n",
      "Epoch 26: 29.72s, train acc=1.00, train loss=0.02, test acc=0.98, test loss=0.07\n",
      "Epoch 27: 30.83s, train acc=0.99, train loss=0.02, test acc=0.98, test loss=0.07\n",
      "Epoch 28: 31.93s, train acc=1.00, train loss=0.02, test acc=0.98, test loss=0.07\n",
      "Epoch 29: 32.98s, train acc=1.00, train loss=0.02, test acc=0.98, test loss=0.07\n",
      "Epoch 30: 34.08s, train acc=1.00, train loss=0.02, test acc=0.98, test loss=0.07\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=500, optimizer='momentum', l_rate=4, beta=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.95s, train acc=0.89, train loss=0.41, test acc=0.89, test loss=0.39\n",
      "Epoch 2: 2.61s, train acc=0.90, train loss=0.33, test acc=0.91, test loss=0.32\n",
      "Epoch 3: 4.08s, train acc=0.91, train loss=0.31, test acc=0.92, test loss=0.29\n",
      "Epoch 4: 5.68s, train acc=0.92, train loss=0.29, test acc=0.92, test loss=0.28\n",
      "Epoch 5: 6.98s, train acc=0.92, train loss=0.28, test acc=0.92, test loss=0.27\n",
      "Epoch 6: 8.40s, train acc=0.92, train loss=0.27, test acc=0.92, test loss=0.26\n",
      "Epoch 7: 9.84s, train acc=0.93, train loss=0.26, test acc=0.92, test loss=0.26\n",
      "Epoch 8: 11.34s, train acc=0.93, train loss=0.25, test acc=0.92, test loss=0.25\n",
      "Epoch 9: 12.79s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n",
      "Epoch 10: 14.50s, train acc=0.93, train loss=0.24, test acc=0.93, test loss=0.25\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
