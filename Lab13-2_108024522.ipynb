{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Documents\\python\n",
      "D:\\python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('D:\\python')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './words_captcha/spec_train_val.txt'\n",
    "input_file = open(data_path, 'r')\n",
    "image_names = [] \n",
    "record_list = []\n",
    "for line in input_file:\n",
    "    line = line.strip()\n",
    "    sp = line.split(' ')\n",
    "    image_names.append('./words_captcha/' + sp[0] + '.png')\n",
    "    record_list.append(['<start>']+list(sp[1])+['<end>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_name = image_names[:100000]\n",
    "train_record_list = record_list[:100000]\n",
    "val_image_name = image_names[100000:]\n",
    "val_record_list = record_list[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_record_list)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "train_ = tokenizer.texts_to_sequences(train_record_list)\n",
    "val_ = tokenizer.texts_to_sequences(val_record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length = calc_max_length(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.keras.preprocessing.sequence.pad_sequences(train_, padding='post',maxlen=max_length)\n",
    "val = tf.keras.preprocessing.sequence.pad_sequences(val_, padding='post',maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_train_steps = len(train) // BATCH_SIZE\n",
    "num_val_steps = len(val) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path,record_list):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (224,224))\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = (image/255) * 2 - 1\n",
    "    return image, record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((train_image_name, train))\n",
    "dataset_train = dataset_train.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_train = dataset_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((val_image_name, val))\n",
    "dataset_val = dataset_val.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_val = dataset_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim,**kwargs):\n",
    "        super(CNN_Encoder, self).__init__(**kwargs)\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, strides=1,activation=\"relu\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3, strides=1,activation=\"relu\")\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, strides=1,activation=\"relu\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 3, strides=1,activation=\"relu\")\n",
    "        self.conv5 = tf.keras.layers.Conv2D(64, 3, strides=1,activation=\"relu\")\n",
    "        self.conv6 = tf.keras.layers.Conv2D(128, 3, strides=1,activation=\"relu\")\n",
    "        self.conv7 = layers.Conv2D(embedding_dim, 3, strides=1,activation=\"relu\")\n",
    "\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.pool4 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_13-02-try5\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "val_acc_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(img_tensor, target):\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    pred_result = tf.zeros((target.shape[0], 1),dtype=tf.float32)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        dec_input = tf.expand_dims(tf.cast(tf.argmax(predictions, axis=1),tf.float32), 1)\n",
    "        \n",
    "        pred_result = tf.concat((pred_result,dec_input),axis=1)\n",
    "    \n",
    "    Target = target[:,1:]\n",
    "    Pred = pred_result[:,1:]\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(Target, 0))\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = Pred.dtype)\n",
    "    Target = tf.cast(Target, dtype = Pred.dtype)\n",
    "    \n",
    "    Pred *= mask\n",
    "    \n",
    "    is_the_same = tf.reduce_all(tf.math.equal(Pred, Target), axis=1)\n",
    "    acc = tf.math.reduce_mean(tf.cast(is_the_same, tf.float32))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "Train Loss: 1.7574057579040527 \\ Validation Accuracy: 0.00019999999494757503\n",
      "Time taken for 1 epoch 1639.8656494617462 sec\n",
      "\n",
      "Epoch 2 \n",
      "Train Loss: 1.5401090383529663 \\ Validation Accuracy: 0.007799995131790638\n",
      "Time taken for 1 epoch 321.63067531585693 sec\n",
      "\n",
      "Epoch 3 \n",
      "Train Loss: 0.7254821062088013 \\ Validation Accuracy: 0.4276999235153198\n",
      "Time taken for 1 epoch 321.8474566936493 sec\n",
      "\n",
      "Epoch 4 \n",
      "Train Loss: 0.30197519063949585 \\ Validation Accuracy: 0.633699893951416\n",
      "Time taken for 1 epoch 322.04762148857117 sec\n",
      "\n",
      "Epoch 5 \n",
      "Train Loss: 0.16273635625839233 \\ Validation Accuracy: 0.7433498501777649\n",
      "Time taken for 1 epoch 322.08063197135925 sec\n",
      "\n",
      "Epoch 6 \n",
      "Train Loss: 0.1146981343626976 \\ Validation Accuracy: 0.7929000854492188\n",
      "Time taken for 1 epoch 322.9812400341034 sec\n",
      "\n",
      "Epoch 7 \n",
      "Train Loss: 0.10251742601394653 \\ Validation Accuracy: 0.8279998302459717\n",
      "Time taken for 1 epoch 321.9898319244385 sec\n",
      "\n",
      "Epoch 8 \n",
      "Train Loss: 0.07154557853937149 \\ Validation Accuracy: 0.8303501605987549\n",
      "Time taken for 1 epoch 322.34735345840454 sec\n",
      "\n",
      "Epoch 9 \n",
      "Train Loss: 0.06035396829247475 \\ Validation Accuracy: 0.8597999811172485\n",
      "Time taken for 1 epoch 322.3393507003784 sec\n",
      "\n",
      "Epoch 10 \n",
      "Train Loss: 0.05333339795470238 \\ Validation Accuracy: 0.8628502488136292\n",
      "Time taken for 1 epoch 321.97247552871704 sec\n",
      "\n",
      "Epoch 11 \n",
      "Train Loss: 0.08306428045034409 \\ Validation Accuracy: 0.8602997064590454\n",
      "Time taken for 1 epoch 322.0559651851654 sec\n",
      "\n",
      "Epoch 12 \n",
      "Train Loss: 0.046775590628385544 \\ Validation Accuracy: 0.8633498549461365\n",
      "Time taken for 1 epoch 321.756959438324 sec\n",
      "\n",
      "Epoch 13 \n",
      "Train Loss: 0.0403570681810379 \\ Validation Accuracy: 0.8826500177383423\n",
      "Time taken for 1 epoch 321.7725865840912 sec\n",
      "\n",
      "Epoch 14 \n",
      "Train Loss: 0.03606794774532318 \\ Validation Accuracy: 0.8866001963615417\n",
      "Time taken for 1 epoch 321.8390736579895 sec\n",
      "\n",
      "Epoch 15 \n",
      "Train Loss: 0.04030890762805939 \\ Validation Accuracy: 0.9125000238418579\n",
      "Time taken for 1 epoch 321.7315320968628 sec\n",
      "\n",
      "Epoch 16 \n",
      "Train Loss: 0.03051942028105259 \\ Validation Accuracy: 0.9052497148513794\n",
      "Time taken for 1 epoch 321.63897347450256 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 16\n",
    "best_acc = 0\n",
    "best_step = 0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print('Epoch {} '.format(epoch + 1))\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_acc_val = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset_train):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "    for (batch_val, (img_tensor_val, target_val)) in enumerate(dataset_val):\n",
    "        acc_val = validation_step(img_tensor_val, target_val)\n",
    "        total_acc_val += acc_val\n",
    "        \n",
    "    curr_val_acc = total_acc_val/num_val_steps\n",
    "    \n",
    "    loss_plot.append(total_loss / num_train_steps)\n",
    "    val_acc_plot.append(curr_val_acc)\n",
    "        \n",
    "    ckpt_manager.save(checkpoint_number=epoch+1)\n",
    "\n",
    "    print ('Train Loss: {} \\ Validation Accuracy: {}'.format(total_loss/num_train_steps, curr_val_acc))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_name = []\n",
    "test = []\n",
    "for i in range(120000,140000):\n",
    "    test_image_name.append('./words_captcha/' + 'a' + str(i) + '.png')\n",
    "    test.append('<start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((test_image_name, test))\n",
    "dataset_test = dataset_test.map(load_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('./Lab13-2_108024522.txt', 'w')\n",
    "for (batch_test, (img_tensor_test, target_test)) in enumerate(dataset_test):\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    features = encoder(img_tensor_test)\n",
    "\n",
    "    pred_result = tf.zeros((target_test.shape[0], 1),dtype=tf.float32)\n",
    "    for i in range(1, max_length):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        dec_input = tf.expand_dims(tf.cast(tf.argmax(predictions, axis=1),tf.float32), 1)\n",
    "\n",
    "        pred_result = tf.concat((pred_result,dec_input),axis=1)\n",
    "        \n",
    "    pred = pred_result[:,1:]\n",
    "    mask = tf.math.logical_not(tf.math.equal(pred, 0))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        output = 'a'+str(batch_test*BATCH_SIZE+i+120000)\n",
    "        pred_index = tf.cast(pred[i],dtype = tf.int32).numpy()\n",
    "        pred_index_clear = []\n",
    "        \n",
    "        for j in (pred_index):\n",
    "            if(j == 2):\n",
    "                break\n",
    "                \n",
    "            pred_index_clear.append(j)\n",
    "            \n",
    "        pred_str = [tokenizer.index_word[j] for j in pred_index_clear]\n",
    "        output =output+' '+(''.join(pred_str))\n",
    "        output_file.write(output+'\\n')\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bita995eddfae774a68a672a54bb067829d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
